# ðŸ“¦ Task-3: Hypertable Optimization with TimescaleDB

## ðŸ“Œ Objective

Optimize query performance for large-scale Fitbit time series data by creating **hypertables** and **continuous aggregates** using **TimescaleDB**.  
This enables efficient storage, indexing, and aggregation over time at multiple resolutions (1-minute, 1-hour, 1-day).

---

## ðŸ§± Tech Stack

- **PostgreSQL** + **TimescaleDB**
- **Python** for ingestion script
- **Docker** for orchestration

---

## ðŸ“ Directory Structure
```Task-3/
â”œâ”€â”€ hypertables.sql          # SQL to create hypertables and aggregates
â”œâ”€â”€ ingestion_update.py      # Python script to insert raw + rollup into aggregates
â”œâ”€â”€ requirements.txt         # psycopg2, pandas, python-dotenv
â”œâ”€â”€ Dockerfile               # Docker container for ingestion_update
```
---

## ðŸ› ï¸ Hypertable Setup

### File: `hypertables.sql`

Creates the following tables:

- `raw_data` â€“ Primary hypertable
- `data_1m` â€“ Aggregated view (1-minute)
- `data_1h` â€“ Aggregated view (1-hour)
- `data_1d` â€“ Aggregated view (1-day)

> All aggregates include `avg(value)` over time per `user_id` and `metric`.

### Run once inside TimescaleDB container:

```bash
docker cp Task-3/hypertables.sql timescaledb:/hypertables.sql
docker exec -it timescaledb psql -U wearipedia_user -d wearipedia -f /hypertables.sql

```

ðŸš€ Aggregated Ingestion Pipeline

File: ingestion_update.py
	â€¢ Loads each metric JSON file from extracted_data/
	â€¢ Inserts data into raw_data
	â€¢ Computes aggregates in Python (pandas) and inserts into:
	â€¢ data_1m
	â€¢ data_1h
	â€¢ data_1d

ðŸ§  Design Decisions
	â€¢ Used pandas for performant grouping and mean aggregation.
	â€¢ Ingestion script is modular and robust against file errors.
	â€¢ JSON data is dynamically read and automatically routed to the correct hypertable and aggregate.
	â€¢ Continuous aggregates enable API to dynamically query appropriate granularity (used in Task-2).